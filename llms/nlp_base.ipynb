{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8270fec-1b93-4f96-bf86-65942c31b532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m481.4/481.4 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, safetensors, regex, fsspec, huggingface-hub, tokenizers, accelerate, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed accelerate-1.6.0 fsspec-2025.3.2 huggingface-hub-0.30.2 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.51.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f471cd7e-e674-4f4d-b060-60714bc3a1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch ë²„ì „: 2.1.0+cu118\n",
      "GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€: True\n",
      "GPU ì´ë¦„: NVIDIA RTX A4000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch ë²„ì „:\", torch.__version__)\n",
    "print(\"GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU ì´ë¦„:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ca578b-60c1-49ed-878a-569c91012549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ee4ac4-0796-41ca-bf9b-c25696f3b9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230255136c5f4e2b92703f79a5758906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): Phi3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                            torch_dtype=torch.float16)\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ae9b633-892a-4fba-a936-04912cbd4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_a(input_text):\n",
    "    return f\"\"\"\n",
    "ë‹¤ìŒ ë¬¸ì¥ì„ ë¶„ì„í•˜ì—¬ intent, amount, recipient, responseë¥¼ ì˜ˆì‹œ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "intentëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤: transfer, confirm, cancel, other  \n",
    "amountëŠ” ìˆ«ìë§Œ (ì—†ìœ¼ë©´ None)  \n",
    "recipientëŠ” ì‚¬ëŒ ì´ë¦„ ë“± (ì—†ìœ¼ë©´ None)  \n",
    "responseëŠ” ê³ ê°ë‹˜ì—ê²Œ ì•ˆë‚´í•  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥\n",
    "\n",
    "ì˜ˆì‹œ:  \n",
    "text: \"ì•„ë¹ í•œí…Œ ì˜¤ë§Œì› ë³´ë‚´ì¤˜\"  \n",
    "ì¶œë ¥:{{\"intent\": \"transfer\",\"amount\": 30000,\"recipient\": \"ì—„ë§ˆ\",\"response\": \"ì—„ë§ˆë‹˜ê»˜ 30,000ì›ì„ ì†¡ê¸ˆí•´ë“œë¦´ê¹Œìš”?\"}}  \n",
    "\n",
    "text: \"{input_text}\"\n",
    "\"\"\"\n",
    "\n",
    "def prompt_b(input_text):\n",
    "    return f\"\"\"\n",
    "ë‹¤ìŒ ë¬¸ì¥ì„ ë¶„ì„í•˜ì—¬ intent, amount, recipient, responseë¥¼ JSONí˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "intentëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤:  \n",
    "  transfer: ì‚¬ìš©ìê°€ ëˆ„êµ°ê°€ì—ê²Œ ê¸ˆì „ì„ ë³´ë‚´ê³ ì í•˜ëŠ” ì˜ë„,  \n",
    "  confirm: ì´ì „ ë°œí™”ë¥¼ í™•ì¸í•˜ê±°ë‚˜ ë°˜ë³µí•˜ëŠ” í‘œí˜„,  \n",
    "  cancel: ì´ì „ ë™ì‘ì„ ì·¨ì†Œí•˜ê±°ë‚˜ ê±°ì ˆí•˜ë ¤ëŠ” ì˜ë„,  \n",
    "  inquiry: ì†¡ê¸ˆê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ë¬»ê±°ë‚˜ í™•ì¸í•˜ëŠ” ì˜ë„,  \n",
    "  other: ì‹œìŠ¤í…œê³¼ ë¬´ê´€í•œ ì¼ìƒ ëŒ€í™” or ë¶„ë¥˜ ë¶ˆê°€ëŠ¥í•œ ë¬¸ì¥,  \n",
    "  system_response: ì‹œìŠ¤í…œì´ ì‚¬ìš©ìì—ê²Œ ì¬ì§ˆë¬¸í•˜ê±°ë‚˜ ì•ˆë‚´í•˜ëŠ” ì‘ë‹µ ë°œí™”  \n",
    "amountëŠ” ìˆ«ìë§Œ (ì—†ìœ¼ë©´ None)  \n",
    "recipientëŠ” ì‚¬ëŒ ì´ë¦„ ë“± (ì—†ìœ¼ë©´ None)  \n",
    "responseëŠ” ê³ ê°ë‹˜ì—ê²Œ ì•ˆë‚´í•  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥, Noneê°’ì´ ìˆë‹¤ë©´ ê¼¬ë¦¬ì§ˆë¬¸ì„ í•´ì„œ noneê°’ì„ ì±„ìš°ì„¸ìš”\n",
    "\n",
    "ì˜ˆì‹œ:  \n",
    "text: \"ì•„ë¹ í•œí…Œ ì˜¤ë§Œì› ë³´ë‚´ì¤˜\"  \n",
    "{{\"intent\": \"transfer\",\"amount\": 30000,\"recipient\": \"ì—„ë§ˆ\",\"response\": \"ì—„ë§ˆë‹˜ê»˜ 30,000ì›ì„ ì†¡ê¸ˆí•´ë“œë¦´ê¹Œìš”?\"}}  \n",
    "\n",
    "text: \"{input_text}\"\n",
    "\"\"\"\n",
    "\n",
    "def prompt_c(input_txt):\n",
    "    return  f\"\"\"\n",
    "    ë‹¤ìŒ ë¬¸ì¥ì„ ë¶„ì„í•˜ì—¬ intent, amount, recipient, responseë¥¼ ì˜ˆì‹œ í˜•ì‹ì„ ë”°ë¼ ì¶”ì¶œí•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "    intentëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤: transfer, confirm, cancel, ì†¡ê¸ˆ ê´€ë ¨ ì •ë³´ë¥¼ ë¬¼ì„ ì‹œ inquiry,ì†¡ê¸ˆ ì™¸ ì§ˆë¬¸ì„ í•  ì‹œ other\n",
    "    amountëŠ” ìˆ«ìë§Œ (ì—†ìœ¼ë©´ None)\n",
    "    recipientëŠ” ì‚¬ëŒ ì´ë¦„ ë“± (ì—†ìœ¼ë©´ None)\n",
    "    responseëŠ” ê³ ê°ë‹˜ì—ê²Œ ì•ˆë‚´í•  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥, Noneê°’ì´ ìˆë‹¤ë©´ ì§ˆë¬¸ì„ í•´ì„œ ê°’ì„ ì±„ìš¸ ê²ƒ\n",
    "\n",
    "ì˜ˆì‹œ:\n",
    "text: \"ì—„ë§ˆí•œí…Œ ì‚¼ë§Œì› ë³´ë‚´ì¤˜\"\n",
    "\n",
    "  {{\"intent\": \"transfer\",\"amount\": 30000,\"recipient\": \"ì—„ë§ˆ\",\"response\": \"ì—„ë§ˆë‹˜ê»˜ 30,000ì›ì„ ì†¡ê¸ˆí•´ë“œë¦´ê¹Œìš”?\"}}\n",
    "\n",
    "\n",
    "{input_text}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d16329ab-940a-42fc-9149-3da3280739cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_inference.py\n",
    "def run_inference(prompt: str, tokenizer, model):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    start = time.time()\n",
    "    outputs = model.generate(**inputs, max_new_tokens=80,do_sample=False)\n",
    "    end = time.time()\n",
    "\n",
    "    # ì¶œë ¥ í•´ì„\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    output_text = generated.replace(prompt, \"\").strip()\n",
    "\n",
    "    print(\"\\nğŸ“¦ LLM ì›ë¬¸ ì¶œë ¥:\")\n",
    "    print(output_text)\n",
    "\n",
    "\n",
    "    try:\n",
    "        json_start = output_text.find(\"{\")\n",
    "        json_end = output_text.rfind(\"}\") + 1\n",
    "        json_block = output_text[json_start:json_end]\n",
    "        parsed = json.loads(json_block)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"JSON íŒŒì‹± ì‹¤íŒ¨:\", e)\n",
    "        print(\"ğŸ“¦ ì›ë¬¸ ì¶œë ¥:\", repr(output_text))\n",
    "        parsed = {}\n",
    "    \n",
    "    print(f\"\\nâ±ï¸ ì²˜ë¦¬ ì‹œê°„: {round(end - start, 2)}ì´ˆ\")\n",
    "    return parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e64c14e4-630e-49c7-b557-c2112b89902c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7a5871e1a441b6acd6061563c06339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 15.63 GiB of which 189.69 MiB is free. Process 3977112 has 15.44 GiB memory in use. Of the allocated memory 15.23 GiB is allocated by PyTorch, and 13.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 2. ì…ë ¥ ë¬¸ì¥\u001b[39;00m\n\u001b[1;32m     12\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mì•„ë¹ í•œí…Œ ì˜¤ì²œ ì› ë³´ë‚´ì¤˜\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3698\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3694\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3695\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3696\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3697\u001b[0m         )\n\u001b[0;32m-> 3698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 15.63 GiB of which 189.69 MiB is free. Process 3977112 has 15.44 GiB memory in use. Of the allocated memory 15.23 GiB is allocated by PyTorch, and 13.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from prompt_templates import prompt_a\n",
    "# from run_inference import run_inference\n",
    "\n",
    "# # 1. ëª¨ë¸ ë¡œë”©\n",
    "# model_name = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model.to(\"cuda\")\n",
    "\n",
    "# 2. ì…ë ¥ ë¬¸ì¥\n",
    "text = \"ì•„ë¹ í•œí…Œ ì˜¤ì²œ ì› ë³´ë‚´ì¤˜\"\n",
    "prompt = prompt_a(text)\n",
    "\n",
    "# 3. ë‹¨ë… ì¸í¼ëŸ°ìŠ¤ ì‹¤í–‰\n",
    "result = run_inference(prompt, tokenizer, model)\n",
    "\n",
    "print(\"\\nğŸ¯ ìµœì¢… ì¶œë ¥ ê²°ê³¼:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b11cf255-ba3a-418f-a737-6373cf165a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_prompt.py\n",
    "\n",
    "def evaluate_prompt(prompt_func, input_data,tokenizer,model):\n",
    "    if isinstance(input_data, dict):\n",
    "        input_data = [input_data]\n",
    "\n",
    "    results = []\n",
    "    for s in input_data:\n",
    "        prompt = prompt_func(s[\"text\"])\n",
    "        output = run_inference(prompt,tokenizer,model)\n",
    "        results.append(output)\n",
    "\n",
    "        # ğŸ”¥  ê²°ê³¼ ì¶œë ¥!\n",
    "        print(f\"\\n[ì…ë ¥ë¬¸ì¥] {s['text']}\")\n",
    "        print(\"ì •ë‹µ:\", s[\"intent\"], s.get(\"slots\", {}))\n",
    "        print(\"ì¶œë ¥:\")\n",
    "        pprint(output)\n",
    "\n",
    "    def clean(s): return str(s).strip().lower() if s else \"\"\n",
    "    def to_int(s): \n",
    "        try: return int(str(s).replace(\",\", \"\").strip())\n",
    "        except: return None\n",
    "\n",
    "    total = len(input_data)\n",
    "\n",
    "    metrics = {\n",
    "        \"intent_acc\": sum(clean(r.get(\"intent\")) == clean(s[\"intent\"]) for r, s in zip(results, input_data)) / total,\n",
    "        \"amount_acc\": sum(to_int(r.get(\"amount\")) == s[\"slots\"].get(\"amount\") for r, s in zip(results, input_data)) / total,\n",
    "        \"recipient_acc\": sum(clean(r.get(\"recipient\")) == clean(s[\"slots\"].get(\"recipient\")) for r, s in zip(results, input_data)) / total,\n",
    "        \"json_success_rate\": sum(bool(r) for r in results) / total\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"metrics\": metrics,\n",
    "        \"results\": results\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c426ed9-98cd-4699-94e4-f34596bcb9d0",
   "metadata": {},
   "source": [
    "### run inference ì‘ì„± ì™„ -> testing í•˜ë©´ ë¨\n",
    "ì•„ë˜ëŠ” ê²€í† í•  ê²ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c293b-0d8d-419e-8aa0-32a9c9aef35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… evaluate_prompt.py\n",
    "\n",
    "def evaluate_prompt(prompt_func, input_data, tokenizer, model):\n",
    "    if isinstance(input_data, dict):\n",
    "        input_data = [input_data]\n",
    "\n",
    "    results = []\n",
    "    total_time = 0\n",
    "    each_times = []\n",
    "\n",
    "    for s in input_data:\n",
    "        prompt = prompt_func(s[\"text\"])\n",
    "        output, elapsed_time = run_inference(prompt, tokenizer, model)\n",
    "        results.append(output)\n",
    "        each_times.append(elapsed_time)\n",
    "        total_time += elapsed_time\n",
    "\n",
    "        print(f\"\\n[ì…ë ¥ë¬¸ì¥] {s['text']}\")\n",
    "        print(\"ğŸ¯ ì •ë‹µ:\", s[\"intent\"], s.get(\"slots\", {}))\n",
    "        print(\"ğŸ§  ì¶œë ¥:\", output)\n",
    "        print(f\"â±ï¸ ì†Œìš” ì‹œê°„: {elapsed_time}ì´ˆ\")\n",
    "\n",
    "    def clean(s): return str(s).strip().lower() if s else \"\"\n",
    "    def to_int(s):\n",
    "        try: return int(str(s).replace(\",\", \"\").strip())\n",
    "        except: return None\n",
    "\n",
    "    total = len(input_data)\n",
    "\n",
    "    metrics = {\n",
    "        \"intent_acc\": sum(clean(r.get(\"intent\")) == clean(s[\"intent\"]) for r, s in zip(results, input_data)) / total,\n",
    "        \"amount_acc\": sum(to_int(r.get(\"amount\")) == s[\"slots\"].get(\"amount\") for r, s in zip(results, input_data)) / total,\n",
    "        \"recipient_acc\": sum(clean(r.get(\"recipient\")) == clean(s[\"slots\"].get(\"recipient\")) for r, s in zip(results, input_data)) / total,\n",
    "        \"json_success_rate\": sum(bool(r) for r in results) / total,\n",
    "        \"avg_response_time\": round(total_time / total, 2)\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"metrics\": metrics,\n",
    "        \"results\": results,\n",
    "        \"times\": each_times\n",
    "    }\n",
    "\n",
    "\n",
    "# âœ… main.py (ì‚¬ìš© ì˜ˆì‹œ)\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from prompt_templates import prompt_a, prompt_b\n",
    "# from evaluate_prompt import evaluate_prompt\n",
    "# from run_inference import run_inference\n",
    "\n",
    "# model_name = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model.to(\"cuda\")\n",
    "\n",
    "# samples = [\n",
    "#     {\n",
    "#         \"text\": \"ì—„ë§ˆí•œí…Œ ì‚¼ë§Œ ì› ë³´ë‚´ì¤˜\",\n",
    "#         \"intent\": \"transfer\",\n",
    "#         \"slots\": {\"recipient\": \"ì—„ë§ˆ\", \"amount\": 30000}\n",
    "#     },\n",
    "#     {\n",
    "#         \"text\": \"ê·¸ëƒ¥ ë„˜ì–´ê°€\",\n",
    "#         \"intent\": \"cancel\",\n",
    "#         \"slots\": {}\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# result_a = evaluate_prompt(prompt_a, samples, tokenizer, model)\n",
    "# result_b = evaluate_prompt(prompt_b, samples, tokenizer, model)\n",
    "\n",
    "# print(\"\\nğŸ“Š í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ë¹„êµ:\")\n",
    "# for k in result_a[\"metrics\"]:\n",
    "#     print(f\"{k:<18}: A={result_a['metrics'][k]*100:.1f}%   |   B={result_b['metrics'][k]*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ddbd999-12dc-49b8-8f53-d74727520cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=[\n",
    "  {\n",
    "    \"text\": \"ì² ìˆ˜í•œí…Œ ì˜¤ì²œ ì› ë³´ë‚´ì¤˜\",\n",
    "    \"intent\": \"ì†¡ê¸ˆ\",\n",
    "    \"slots\": { \"recipient\": \"ì² ìˆ˜\", \"amount\": 5000 }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"ì—„ë§ˆí•œí…Œ 3ë§Œì›ë§Œ\",\n",
    "    \"intent\": \"ì†¡ê¸ˆ\",\n",
    "    \"slots\": { \"recipient\": \"ì—„ë§ˆ\", \"amount\": 30000 }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"ë§Œì›ë§Œ ë³´ë‚´ì¤˜\",\n",
    "    \"intent\": \"ì†¡ê¸ˆ\",\n",
    "    \"slots\": { \"recipient\": None, \"amount\": 10000 }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"ì—„ë§ˆí•œí…Œ ë³´ë‚´ì¤˜\",\n",
    "    \"intent\": \"ì†¡ê¸ˆ\",\n",
    "    \"slots\": { \"recipient\": \"ì—„ë§ˆ\", \"amount\": None }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"ë‚˜ ì†¡ê¸ˆí•˜ê³  ì‹¶ì–´\",\n",
    "    \"intent\": \"ì†¡ê¸ˆ\",\n",
    "    \"slots\": { \"recipient\": None, \"amount\": None }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"ê°€ìŠ¤ë¹„ ë‚´ì•¼ í•˜ëŠ”ë° ì–´ë–»ê²Œ í•´ì•¼ í•´?\",\n",
    "    \"intent\": \"ì§ˆë¬¸\",\n",
    "    \"slots\": { \"topic\": \"ê°€ìŠ¤ë¹„\" }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"ë‚´ ê³„ì¢Œì— ì–¼ë§ˆ ë‚¨ì•˜ì–´?\",\n",
    "    \"intent\": \"ì§ˆë¬¸\",\n",
    "    \"slots\": { \"topic\": \"ì”ì•¡ì¡°íšŒ\" }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"ê·¸ ì‚¬ëŒí•œí…Œ ë˜ ë³´ë‚´ì¤˜\",\n",
    "    \"intent\": \"ì†¡ê¸ˆ\",\n",
    "    \"slots\": { \"recipient\": \"ì´ì „ëŒ€í™”\", \"amount\": \"ì´ì „ëŒ€í™”\" }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"ì•„ê¹Œë‘ ë˜‘ê°™ì´ ë³´ë‚´ì¤˜\",\n",
    "    \"intent\": \"ì†¡ê¸ˆ\",\n",
    "    \"slots\": { \"recipient\": \"ì´ì „ëŒ€í™”\", \"amount\": \"ì´ì „ëŒ€í™”\" }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"ì†¡ê¸ˆí• ë˜\",\n",
    "    \"intent\": \"ì†¡ê¸ˆ\",\n",
    "    \"slots\": { \"recipient\": None, \"amount\": None }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"ì´ì²´ ìˆ˜ìˆ˜ë£Œ ì–¼ë§ˆì•¼?\",\n",
    "    \"intent\": \"ì§ˆë¬¸\",\n",
    "    \"slots\": { \"topic\": \"ì´ì²´ ìˆ˜ìˆ˜ë£Œ\" }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"ì˜¤ëŠ˜ ì•„ë¹ ë‘ ì ì‹¬ ë¨¹ì—ˆì–´\",\n",
    "    \"intent\": \"ê¸°íƒ€\",\n",
    "    \"slots\": {}\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"ì•„ë¹ í•œí…Œ ì´ë§Œì› ë³´ë‚´ì¤˜\",\n",
    "    \"intent\": \"ì†¡ê¸ˆ\",\n",
    "    \"slots\": { \"recipient\": \"ì•„ë¹ \", \"amount\": 20000 }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"ì—¬ë³´í•œí…Œ ì‹­ë§Œ ì› ì´ì²´í•´ì¤˜\",\n",
    "    \"intent\": \"ì†¡ê¸ˆ\",\n",
    "    \"slots\": { \"recipient\": \"ì—¬ë³´\", \"amount\": 100000 }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"ë³´ë‚´ì§€ ë§ˆ\",\n",
    "    \"intent\": \"ì·¨ì†Œ\",\n",
    "    \"slots\": {}\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"ê·¸ëƒ¥ ë„˜ì–´ê°€ì¤˜\",\n",
    "    \"intent\": \"ì·¨ì†Œ\",\n",
    "    \"slots\": {}\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"ì—„ë§ˆ ì†¡ê¸ˆí•´ì¤˜\",\n",
    "    \"intent\": \"ì†¡ê¸ˆ\",\n",
    "    \"slots\": { \"recipient\": \"ì—„ë§ˆ\", \"amount\": None }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"ì•„, ì‚¼ë§Œì› ë³´ë‚´ëŠ” ê±°ì˜€ì§€\",\n",
    "    \"intent\": \"í™•ì¸\",\n",
    "    \"slots\": { \"amount\": 30000 }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"ì˜ˆê¸ˆí•´ì¤˜\",\n",
    "    \"intent\": \"ê¸°íƒ€\",\n",
    "    \"slots\": { \"action\": \"ì˜ˆê¸ˆ\" }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"ëˆ„êµ¬í•œí…Œ ë³´ë‚¼ê¹Œìš”?\",\n",
    "    \"intent\": \"ì‹œìŠ¤í…œì‘ë‹µ\",\n",
    "    \"slots\": {}\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42be50d-3337-4e2a-9f3f-1f7254435f8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_a = evaluate_prompt(prompt_a, samples[0])\n",
    "result_b = evaluate_prompt(prompt_b, samples[0])\n",
    "\n",
    "print(\"\\nğŸ“Š í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ë¹„êµ:\")\n",
    "for k in result_a[\"metrics\"]:\n",
    "    print(f\"{k:<18}: A={result_a['metrics'][k]*100:.1f}%   |   B={result_b['metrics'][k]*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c678dcce-2bac-48e6-9b0c-c87649b7f851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] ë¬¸ì¥: ì² ìˆ˜í•œí…Œ ì˜¤ì²œ ì› ë³´ë‚´ì¤˜\n",
      "âœ… íŒŒì‹±ëœ ê²°ê³¼:\n",
      "{\n",
      "  \"intent\": \"transfer\",\n",
      "  \"amount\": 100000,\n",
      "  \"recipient\": \"ì² ìˆ˜\",\n",
      "  \"response\": \"ì² ìˆ˜ë‹˜ê»˜ ì²œì›ì„ ì†¡ê¸ˆí•´ë“œë¦´ê¹Œìš”?\"\n",
      "}\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: 4.07ì´ˆ\n",
      "\n",
      "[2] ë¬¸ì¥: ì—„ë§ˆí•œí…Œ 3ë§Œì›ë§Œ\n",
      "âŒ JSON íŒŒì‹± ì‹¤íŒ¨: Expecting value: line 1 column 1 (char 0)\n",
      "ğŸ“¦ ì›ë¬¸ ì¶œë ¥: 'text: \"ì €ëŠ” ì†¡ê¸ˆ ì˜ˆì •ì´ì—ìš”\"\\n\\n  {\"intent\": \"inquiry\",\"amount\": None,\"recipient\": None,\"response\": \"ì†¡ê¸ˆ ì˜ˆì •ì´ì—ìš”ë„¤ìš”, ì–¸ì œ ì†¡ê¸ˆ'\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: 3.91ì´ˆ\n",
      "\n",
      "[3] ë¬¸ì¥: ë§Œì›ë§Œ ë³´ë‚´ì¤˜\n",
      "âŒ JSON íŒŒì‹± ì‹¤íŒ¨: Expecting value: line 1 column 52 (char 51)\n",
      "ğŸ“¦ ì›ë¬¸ ì¶œë ¥: '{\"intent\": \"transfer\",\"amount\": 10000,\"recipient\": None,\"response\": \"10,000ì›ì„ ì†¡ê¸ˆí•˜ì‹œê² ìŠµë‹ˆê¹Œ?\"}\\n\\n\\nì†¡ê¸ˆí•˜ê³  ì‹¶ì–´ìš”\\n\\n  {\"intent\": \"'\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: 3.94ì´ˆ\n",
      "\n",
      "[4] ë¬¸ì¥: ì—„ë§ˆí•œí…Œ ë³´ë‚´ì¤˜\n",
      "âœ… íŒŒì‹±ëœ ê²°ê³¼:\n",
      "{\n",
      "  \"intent\": \"transfer\",\n",
      "  \"amount\": 10000,\n",
      "  \"recipient\": \"ì—„ë§ˆ\",\n",
      "  \"response\": \"ì—„ë§ˆë‹˜ê»˜ 10,000ì›ì„ ì†¡ê¸ˆí•´ë“œë¦´ê¹Œìš”?\"\n",
      "}\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: 4.09ì´ˆ\n",
      "\n",
      "[5] ë¬¸ì¥: ë‚˜ ì†¡ê¸ˆí•˜ê³  ì‹¶ì–´\n",
      "âŒ JSON íŒŒì‹± ì‹¤íŒ¨: Expecting value: line 1 column 1 (char 0)\n",
      "ğŸ“¦ ì›ë¬¸ ì¶œë ¥: ''\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: 4.11ì´ˆ\n",
      "\n",
      "[6] ë¬¸ì¥: ê°€ìŠ¤ë¹„ ë‚´ì•¼ í•˜ëŠ”ë° ì–´ë–»ê²Œ í•´ì•¼ í•´?\n",
      "âŒ JSON íŒŒì‹± ì‹¤íŒ¨: Expecting value: line 1 column 32 (char 31)\n",
      "ğŸ“¦ ì›ë¬¸ ì¶œë ¥: '{\"intent\": \"inquiry\",\"amount\": None,\"recipient\": None,\"response\": \"ê°€ìŠ¤ë¹„ë¥¼ ì¤€ë¹„í•˜ë ¤ë©´ ì €í¬ì—ê²Œ ë” ìì„¸í•œ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\"}ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½'\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: 4.15ì´ˆ\n",
      "\n",
      "[7] ë¬¸ì¥: ë‚´ ê³„ì¢Œì— ì–¼ë§ˆ ë‚¨ì•˜ì–´?\n",
      "âŒ JSON íŒŒì‹± ì‹¤íŒ¨: Expecting value: line 1 column 32 (char 31)\n",
      "ğŸ“¦ ì›ë¬¸ ì¶œë ¥: '{\"intent\": \"inquiry\",\"amount\": None,\"recipient\": None,\"response\": \"ë‚´ ê³„ì¢Œì—ëŠ” ì–¼ë§ˆë‚˜ ë‚¨ì•˜ë‚˜ìš”?\"}\\n\\n\\nì†¡ê¸ˆí•˜ê³  ì‹¶ì–´ìš”.\\n\\n  {\"intent\": \"transfer\",\"amount\": None,\"'\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: 4.13ì´ˆ\n",
      "\n",
      "[8] ë¬¸ì¥: ê·¸ ì‚¬ëŒí•œí…Œ ë˜ ë³´ë‚´ì¤˜\n",
      "âœ… íŒŒì‹±ëœ ê²°ê³¼:\n",
      "{\n",
      "  \"intent\": \"transfer\",\n",
      "  \"amount\": 10000,\n",
      "  \"recipient\": \"ê·¸ëŸ° ì‚¬ëŒ\",\n",
      "  \"response\": \"ê·¸ëŸ° ì‚¬ëŒë‹˜ê»˜ 10,000ì›ì„ ì†¡ê¸ˆí•´ë“œë¦´ê¹Œìš”?\"\n",
      "}\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: 3.97ì´ˆ\n",
      "\n",
      "[9] ë¬¸ì¥: ì•„ê¹Œë‘ ë˜‘ê°™ì´ ë³´ë‚´ì¤˜\n",
      "âœ… íŒŒì‹±ëœ ê²°ê³¼:\n",
      "{\n",
      "  \"intent\": \"transfer\",\n",
      "  \"amount\": 10000,\n",
      "  \"recipient\": \"ì•„ê¹Œë‘\",\n",
      "  \"response\": \"ì•„ê¹Œë‘ 10,000ì›ì„ ì†¡ê¸ˆí•´ë“œë¦´ê¹Œìš”?\"\n",
      "}\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: 4.02ì´ˆ\n",
      "\n",
      "[10] ë¬¸ì¥: ì†¡ê¸ˆí• ë˜\n",
      "âŒ JSON íŒŒì‹± ì‹¤íŒ¨: Expecting value: line 1 column 1 (char 0)\n",
      "ğŸ“¦ ì›ë¬¸ ì¶œë ¥: 'ì†¡ê¸ˆ ê´€ë ¨ ì •ë³´ë¥¼ ë¬¼ì„ ì‹œ inquiry\\n\\nì†¡ê¸ˆ ê´€ë ¨ ì •ë³´ë¥¼ ë¬¼ì„ ì‹œ inquiry\\n\\nì†¡ê¸ˆ ê´€ë ¨ ì •ë³´ë¥¼ ë¬¼ì„'\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: 4.01ì´ˆ\n",
      "\n",
      "[11] ë¬¸ì¥: ì´ì²´ ìˆ˜ìˆ˜ë£Œ ì–¼ë§ˆì•¼?\n",
      "âŒ JSON íŒŒì‹± ì‹¤íŒ¨: Expecting value: line 1 column 32 (char 31)\n",
      "ğŸ“¦ ì›ë¬¸ ì¶œë ¥: '{\"intent\": \"inquiry\",\"amount\": None,\"recipient\": None,\"response\": \"ì´ ìˆ˜ìˆ˜ë£ŒëŠ” ì–¼ë§ˆì¸ê°€ìš”?\"}\\n\\n\\nì†¡ê¸ˆ ì˜ˆì •ì¼ì€ ëª‡ ì¼ì´ì•¼?\\n\\n  {\"intent\": \"inquiry\",\"amount'\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: 4.11ì´ˆ\n",
      "\n",
      "[12] ë¬¸ì¥: ì˜¤ëŠ˜ ì•„ë¹ ë‘ ì ì‹¬ ë¨¹ì—ˆì–´\n",
      "âœ… íŒŒì‹±ëœ ê²°ê³¼:\n",
      "{\n",
      "  \"intent\": \"other\",\n",
      "  \"response\": \"ì˜¤ëŠ˜ ì•„ë¹ ë‘ ì ì‹¬ ë¨¹ì—ˆì–´ìš”, ì˜ ì§€ëƒˆì–´ìš”?\"\n",
      "}\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: 4.01ì´ˆ\n",
      "\n",
      "[13] ë¬¸ì¥: ì•„ë¹ í•œí…Œ ì´ë§Œì› ë³´ë‚´ì¤˜\n",
      "âœ… íŒŒì‹±ëœ ê²°ê³¼:\n",
      "{\n",
      "  \"intent\": \"transfer\",\n",
      "  \"amount\": 10000,\n",
      "  \"recipient\": \"ì•„ë¹ \",\n",
      "  \"response\": \"ì•„ë¹ ë‹˜ê»˜ 1ë§Œì›ì„ ì†¡ê¸ˆí•´ë“œë¦´ê¹Œìš”?\"\n",
      "}\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: 3.98ì´ˆ\n",
      "\n",
      "[14] ë¬¸ì¥: ì—¬ë³´í•œí…Œ ì‹­ë§Œ ì› ì´ì²´í•´ì¤˜\n",
      "âœ… íŒŒì‹±ëœ ê²°ê³¼:\n",
      "{\n",
      "  \"intent\": \"transfer\",\n",
      "  \"amount\": 100000,\n",
      "  \"recipient\": \"ì—¬ë³´\",\n",
      "  \"response\": \"ì—¬ë³´ë‹˜ê»˜ 100,000ì›ì„ ì†¡ê¸ˆí•´ë“œë¦´ê¹Œìš”?\"\n",
      "}\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: 3.87ì´ˆ\n",
      "\n",
      "[15] ë¬¸ì¥: ë³´ë‚´ì§€ ë§ˆ\n",
      "âŒ JSON íŒŒì‹± ì‹¤íŒ¨: Expecting value: line 1 column 1 (char 0)\n",
      "ğŸ“¦ ì›ë¬¸ ì¶œë ¥: 'ì˜ˆì‹œ:\\ntext: \"ì €í¬ ì†¡ê¸ˆ ì„œë¹„ìŠ¤ ì•Œë ¤ì£¼ì„¸ìš”\"\\n\\n  {\"intent\": \"inquiry\",\"amount\": None,\"recipient\": None,\"response\": \"ì €í¬ ì†¡ê¸ˆ ì„œë¹„ìŠ¤'\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: 3.89ì´ˆ\n",
      "\n",
      "[16] ë¬¸ì¥: ê·¸ëƒ¥ ë„˜ì–´ê°€ì¤˜\n",
      "âŒ JSON íŒŒì‹± ì‹¤íŒ¨: Expecting value: line 1 column 1 (char 0)\n",
      "ğŸ“¦ ì›ë¬¸ ì¶œë ¥: 'text: \"ì €í¬ íšŒì‚¬ì—ì„œ ì†¡ê¸ˆí•˜ë ¤ê³  í•©ë‹ˆë‹¤\"\\n\\n  {\"intent\": \"transfer\",\"amount\": None,\"recipient\": None,\"response\": \"ì €í¬ íšŒì‚¬ì—ì„œ ì†¡ê¸ˆí•˜ë ¤ë©´ ï¿½'\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: 3.92ì´ˆ\n",
      "\n",
      "[17] ë¬¸ì¥: ì—„ë§ˆ ì†¡ê¸ˆí•´ì¤˜\n",
      "âŒ JSON íŒŒì‹± ì‹¤íŒ¨: Expecting value: line 1 column 32 (char 31)\n",
      "ğŸ“¦ ì›ë¬¸ ì¶œë ¥: 'text: \"ì €ëŠ” ì†¡ê¸ˆí•˜ê³  ì‹¶ì–´ìš”\"\\n\\n  {\"intent\": \"inquiry\",\"amount\": None,\"recipient\": None,\"response\": \"ì†¡ê¸ˆí•˜ê³  ì‹¶ì€ ì‚¬ëŒì´ ë­”ê°€ìš”?\"}ï¿½ï¿½ï¿½ï¿½'\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: 3.86ì´ˆ\n",
      "\n",
      "[18] ë¬¸ì¥: ì•„, ì‚¼ë§Œì› ë³´ë‚´ëŠ” ê±°ì˜€ì§€\n",
      "âœ… íŒŒì‹±ëœ ê²°ê³¼:\n",
      "{\n",
      "  \"intent\": \"transfer\",\n",
      "  \"amount\": 3000000,\n",
      "  \"recipient\": \"ì—„ë§ˆ\",\n",
      "  \"response\": \"ì—„ë§ˆë‹˜ê»˜ 3,000,000ì›ì„ ì†¡ê¸ˆí•´ë“œë¦´ê¹Œìš”?\"\n",
      "}\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: 3.85ì´ˆ\n",
      "\n",
      "[19] ë¬¸ì¥: ì˜ˆê¸ˆí•´ì¤˜\n",
      "âŒ JSON íŒŒì‹± ì‹¤íŒ¨: Expecting value: line 1 column 1 (char 0)\n",
      "ğŸ“¦ ì›ë¬¸ ì¶œë ¥: 'ì—„ë§ˆí•œí…Œ ì†¡ê¸ˆí•´\\n\\nì†¡ê¸ˆí•´ìš”\\n\\nì†¡ê¸ˆí•´ìš”\\n\\nì†¡ê¸ˆí•´ìš”\\n\\nì†¡ê¸ˆí•´ìš”\\n\\nì†¡ê¸ˆí•´ìš”\\n\\nì†¡ê¸ˆí•´ìš”ï¿½ï¿½ï¿½'\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: 3.86ì´ˆ\n",
      "\n",
      "[20] ë¬¸ì¥: ëˆ„êµ¬í•œí…Œ ë³´ë‚¼ê¹Œìš”?\n",
      "âœ… íŒŒì‹±ëœ ê²°ê³¼:\n",
      "{\n",
      "  \"intent\": \"inquiry\",\n",
      "  \"response\": \"ëˆ„ê°€ ë³´ë‚¼ê¹Œìš”?\"\n",
      "}\n",
      "\n",
      "â±ï¸ ì²˜ë¦¬ ì‹œê°„: 3.87ì´ˆ\n"
     ]
    }
   ],
   "source": [
    "for i,ex in enumerate(sample):\n",
    "    input_text = ex[\"text\"]\n",
    "    print(f\"\\n[{i+1}] ë¬¸ì¥: {input_text}\")\n",
    "\n",
    "    result = run_chatbot_inference(input_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c746fbf-2e90-4329-81f9-fd409c10f11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ intent ì˜¤ë‹µ (ì˜ˆìƒ: ì‹œìŠ¤í…œì‘ë‹µ, ì˜ˆì¸¡: None)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'recipient'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâŒ intent ì˜¤ë‹µ (ì˜ˆìƒ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, ì˜ˆì¸¡: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred_intent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# ëŒ€ìƒ ë¹„êµ\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pred_recipient \u001b[38;5;241m==\u001b[39m \u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslots\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecipient\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     14\u001b[0m     correct_recipient \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ” recipient ì •ë‹µ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'recipient'"
     ]
    }
   ],
   "source": [
    "correct_intent = 0\n",
    "correct_recipient = 0\n",
    "correct_amount = 0\n",
    "    \n",
    "\n",
    "    pred_intent = result.get(\"intent\")\n",
    "    pred_recipient = result.get(\"recipient\")\n",
    "    pred_amount = int(result.get(\"amount\")) if result.get(\"amount\") is not None else None\n",
    "\n",
    "    # ì˜ë„ ë¹„êµ\n",
    "    if pred_intent == ex[\"intent\"]:\n",
    "        correct_intent += 1\n",
    "        print(\"âœ” intent ì •ë‹µ\")\n",
    "    else:\n",
    "        print(f\"âŒ intent ì˜¤ë‹µ (ì˜ˆìƒ: {ex['intent']}, ì˜ˆì¸¡: {pred_intent})\")\n",
    "\n",
    "    # ëŒ€ìƒ ë¹„êµ\n",
    "    if pred_recipient == ex[\"slots\"][\"recipient\"]:\n",
    "        correct_recipient += 1\n",
    "        print(\"âœ” recipient ì •ë‹µ\")\n",
    "    else:\n",
    "        print(f\"âŒ recipient ì˜¤ë‹µ (ì˜ˆìƒ: {ex['slots']['recipient']}, ì˜ˆì¸¡: {pred_recipient})\")\n",
    "\n",
    "    # ê¸ˆì•¡ ë¹„êµ\n",
    "    if pred_amount == ex[\"slots\"][\"amount\"]:\n",
    "        correct_amount += 1\n",
    "        print(\"âœ” amount ì •ë‹µ\")\n",
    "    else:\n",
    "        print(f\"âŒ amount ì˜¤ë‹µ (ì˜ˆìƒ: {ex['slots']['amount']}, ì˜ˆì¸¡: {pred_amount})\")\n",
    "\n",
    "# í‰ê°€ ê²°ê³¼ ì¶œë ¥\n",
    "total = len(sample)\n",
    "print(\"\\nğŸ“Š í‰ê°€ ê²°ê³¼\")\n",
    "print(f\"Intent ì •í™•ë„: {correct_intent}/{total} ({correct_intent/total:.0%})\")\n",
    "print(f\"Recipient ì •í™•ë„: {correct_recipient}/{total} ({correct_recipient/total:.0%})\")\n",
    "print(f\"Amount ì •í™•ë„: {correct_amount}/{total} ({correct_amount/total:.0%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04db84a4-03b5-47ac-b430-f44c37d4a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d2cf00-8bb3-49ce-84b3-50636ac9b433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
