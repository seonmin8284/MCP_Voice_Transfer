# backend/llm/main.py
import time
import json
# from event.redis import publish_event
# llm/send_to_fds.py ë˜ëŠ” fds/main.py
# from proto import intent_pb2, intent_pb2_grpc
# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("microsoft/Phi-3-mini-4k-instruct", trust_remote_code=True,attn_implementation="eager" )

def run_chatbot_inference(input_text:str):
  prompt=f"""
  ì‚¬ìš©ìì˜ ë¬¸ì¥ì„ ë¶„ì„í•˜ì—¬ ì˜ë„(intent), ì—”í‹°í‹°(entity), ì‘ë‹µ ë©”ì‹œì§€ë¥¼ ìƒì„±í•˜ì„¸ìš”.
ì¶œë ¥ í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ JSONìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:
{{
  "intent": "...",
  "amount": ...,
  "recipient": "...",
  "response": "..."
}} 

ë¬¸ì¥: "{input_text}"

"""
  inputs=tokenizer(prompt,return_tensors="pt")
  
  start=time.time()
  outputs=model.generate(
    **inputs,
    max_new_tokens=100,
    do_sample=False,
    temperature=0.7,
    top_p=0.9
  )
  end = time.time()
  generated=tokenizer.decode(outputs[0],skip_special_tokens=True)
  json_str=generated.replace(prompt,"").strip()
  
  try:
    result=json.loads(json_str)
    print("ğŸ§  LLM ê²°ê³¼:\n", result)
    
    #Redis ì´ë²¤íŠ¸ ë°œí–‰
    if result["intent"]=="ì†¡ê¸ˆ":
      # publish_event("intent_detected", {
      #   "user_id": "user1",
      #   "amount": result.get("amount"),
      #   "recipient": result.get("recipient")
      # })
      print(result.get("amount"), result.get("recipient"))

    # ì‚¬ìš©ì ì‘ë‹µ ì¶œë ¥
    print("ğŸ’¬ ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´í•  ë©”ì‹œì§€:\n", result["response"])
  
  except Exception as e:
    print('json íŒŒì‹± ì‹¤íŒ¨:',e)
    print("ì›ë¬¸:",json_str)
  
  print(f"ì²˜ë¦¬ ì‹œê°„:{round(end-start,2)}ì´ˆ")
