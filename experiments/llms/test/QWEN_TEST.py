# %% library load
import sys
import torch
import transformers
import onnx
import onnxruntime
import tokenizers
import numpy

import re
import time
import json

from transformers import AutoModelForCausalLM, AutoTokenizer
# !pip install prompt_templates

# %% data load
with open("samples.json") as f:
    samples=json.load(f)
    
print(samples)

#%%
qwen_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")
qwen = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct",torch_dtype=torch.float16).to("cuda")



#%% prompt_templates.py
def unified_system_prompt(input_text: str) -> list:
    """
    Qwen, GPT ë“± ChatML êµ¬ì¡° ê¸°ë°˜ LLMì— ì…ë ¥í•  ë©”ì‹œì§€ í¬ë§· (system + user).
    ë¶„ì„ + ì‘ë‹µì„ ë™ì‹œì— ìš”ì²­í•©ë‹ˆë‹¤.
    """
    system_message = {
        "role": "system",
        "content": """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ê¸ˆìœµ ë°œí™”ë¥¼ ë¶„ì„í•˜ëŠ” AI ì†¡ê¸ˆ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§€ì¹¨ì— ë”°ë¼ ì‘ë™í•˜ì„¸ìš”:

1. ì‚¬ìš©ìì˜ ë¬¸ì¥ì—ì„œ ë‹¤ìŒ í•­ëª©ì„ ì¶”ì¶œí•˜ì„¸ìš”:
    - intent: ë‹¤ìŒ ì¤‘ í•˜ë‚˜ (transfer, confirm, cancel, inquiry, other, system_response)
    - amount: ìˆ«ìë§Œ ì¶”ì¶œ (ì—†ìœ¼ë©´ null)
    - recipient: ì‚¬ëŒ ì´ë¦„ ë“± (ì—†ìœ¼ë©´ null)

2. ì‚¬ìš©ìì˜ ë°œí™”ì— ì–´ìš¸ë¦¬ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ì•ˆë‚´ ì‘ë‹µ(response)ì„ ìƒì„±í•˜ì„¸ìš”.

3. ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ í•˜ë‚˜ì˜ ê°ì²´ë¡œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.

ì˜ˆì‹œ:
{
  "intent": "transfer",
  "amount": 30000,
  "recipient": "ì—„ë§ˆ",
  "response": "ì—„ë§ˆë‹˜ê»˜ 30,000ì›ì„ ì†¡ê¸ˆí•´ë“œë¦´ê¹Œìš”?"
}
"""
    }

    user_message = {
        "role": "user",
        "content": input_text
    }

    return [system_message, user_message]


def run_inference_qwen(input_text: str, tokenizer, model, max_new_tokens=128):

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ChatML)
    messages = unified_system_prompt(input_text)
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    # í† í¬ë‚˜ì´ì¦ˆ ë° ë””ë°”ì´ìŠ¤ ì´ë™
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # ëª¨ë¸ ì¶”ë¡ 
    start = time.time()
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=False,
        use_cache=False
    )
    end = time.time()

    # ë””ì½”ë”© ë° í”„ë¡¬í”„íŠ¸ ì œê±°
    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    output_text = generated.replace(prompt, "").strip()
    
    # 5. 'assistant' ì´í›„ í…ìŠ¤íŠ¸ë§Œ ë‚¨ê¸°ê¸°
    assistant_split = re.split(r"\bassistant\b", output_text, flags=re.IGNORECASE)
    if len(assistant_split) < 2:
        print("âš ï¸ 'assistant' ì´í›„ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return output_text,None, round(end - start, 2)

    assistant_response = assistant_split[-1].strip()

    
    match = re.search(r'\{\s*"intent":.*?\}', assistant_response, re.DOTALL)
    if match:
        try:
            parsed_json = json.loads(match.group())
            return output_text,parsed_json, round(end - start, 2)
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            return output_text, None, round(end - start, 2)
    else:
        print("âš ï¸ assistant ì´í›„ JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return output_text, parsed_json, round(end - start, 2)

#%%
result, pasing, elapsed = run_inference_qwen("ì•ˆë…•", qwen_tokenizer, qwen)
print("ğŸ” ì¶”ë¡  ê²°ê³¼:", result)
print("ğŸ§© íŒŒì‹±ëœ JSON:\n", pasing)
print("â±ï¸ ì²˜ë¦¬ ì‹œê°„:", elapsed, "ì´ˆ")


#%% JSON PASING & SAVE

parsed = []
raw_outputs=[]
total_time = 0

for sample in samples:
    result, parsing, elapsed = run_inference_qwen(sample["text"], qwen_tokenizer, qwen)
    total_time += elapsed

    parsed = {
        "text": sample["text"],
        "intent": parsing["intent"],
        "slots": {
            "recipient": parsing["recipient"],
            "amount": parsing["amount"]
        },
        "response": "",
        "_meta": {
            "inference_time": elapsed
        }
    }
    
    raw_outputs.append({
    "text": sample["text"],
    "raw_output": result
    })

    # if isinstance(parsed, dict):
    #     result["intent"] = parsed.get("intent")
    #     result["slots"] = parsed.get("slots", {
    #         "recipient": None,
    #         "amount": None
    #     })
    #     result["response"] = parsed.get("response", "")
    # else:
    #     result["_meta"]["error"] = "Parsing failed"

    # results.append(result)

# ì €ì¥
with open("results_qwen.json", "w", encoding="utf-8") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

with open("raw_outputs.json", "w", encoding="utf-8") as f:
    json.dump(raw_outputs, f, indent=2, ensure_ascii=False)


#%% ASSESMENT

# í‰ê°€ ì§€í‘œ ì´ˆê¸°í™”
correct_intent = 0
correct_recipient = 0
correct_amount = 0
parsing_success = 0

for result, ex in zip(results, samples):
    meta = result.get("_meta", {})
    
    # íŒŒì‹± ì„±ê³µ ì—¬ë¶€
    if "error" not in meta:
        parsing_success += 1

    # ì˜ˆì¸¡ê°’
    pred_intent = result.get("intent")
    pred_recipient = result.get("slots", {}).get("recipient")
    pred_amount = result.get("slots", {}).get("amount")

    # ì •ë‹µê°’
    true_intent = ex["intent"]
    true_recipient = ex["slots"]["recipient"]
    true_amount = ex["slots"]["amount"]

    # í‰ê°€
    if pred_intent == true_intent:
        correct_intent += 1
    if pred_recipient == true_recipient:
        correct_recipient += 1
    if pred_amount == true_amount:
        correct_amount += 1

# ì´ ìƒ˜í”Œ ìˆ˜
total = len(samples)

# í‰ê°€ ê²°ê³¼ ì •ë¦¬
evaluation = {
    "Intent ì •í™•ë„": f"{correct_intent}/{total} ({correct_intent/total:.0%})",
    "Recipient ì •í™•ë„": f"{correct_recipient}/{total} ({correct_recipient/total:.0%})",
    "Amount ì •í™•ë„": f"{correct_amount}/{total} ({correct_amount/total:.0%})",
    "íŒŒì‹± ì„±ê³µë¥ ": f"{parsing_success}/{total} ({parsing_success/total:.0%})"
}

print(evaluation)



# %%
