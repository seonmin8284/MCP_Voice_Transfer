# %% library load

# pip install transformers==4.50.0

import sys
import torch
import transformers
# import onnx
# import onnxruntime
import tokenizers
import numpy

import re
import time
import json

from transformers import AutoModelForCausalLM, AutoTokenizer
# !pip install prompt_templates

# %% data load
with open("/workspace/MCP_Voice_Transfer/experiments/llms/test/samples.json") as f:
    samples=json.load(f)
    
print(samples)

#%%
qwen_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")
qwen = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct",torch_dtype=torch.float16).to("cuda")

#%%
qwen_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")
qwen = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct",torch_dtype=torch.float16).to("cuda")

#%% transformers-4.52.0.dev0 |  pip-25.1.1
qwen_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-0.6B-Base")
qwen = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-0.6B-Base")


#%% prompt_templates.py
def unified_system_prompt1(input_text: str) -> list:

    system_message = {
        "role": "system",
        "content": """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ê¸ˆìœµ ë°œí™”ë¥¼ ë¶„ì„í•˜ëŠ” AI ì†¡ê¸ˆ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§€ì¹¨ì— ë”°ë¼ ì‘ë™í•˜ì„¸ìš”:

1. ì‚¬ìš©ìì˜ ë¬¸ì¥ì—ì„œ ë‹¤ìŒ í•­ëª©ì„ ì¶”ì¶œí•˜ì„¸ìš”:
    - intent: ë‹¤ìŒ ì¤‘ í•˜ë‚˜ (transfer, confirm, cancel, inquiry, other, system_response)
    - amount: ìˆ«ìë§Œ ì¶”ì¶œ (ì—†ìœ¼ë©´ null)
    - recipient: ì‚¬ëŒ ì´ë¦„ ë“± (ì—†ìœ¼ë©´ null)

2. ì‚¬ìš©ìì˜ ë°œí™”ì— ì–´ìš¸ë¦¬ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ì•ˆë‚´ ì‘ë‹µ(response)ì„ ìƒì„±í•˜ì„¸ìš”.

3. ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ í•˜ë‚˜ì˜ ê°ì²´ë¡œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.

ì˜ˆì‹œ:
{
  "intent": "transfer",
  "amount": 30000,
  "recipient": "ì—„ë§ˆ",
  "response": "ì—„ë§ˆë‹˜ê»˜ 30,000ì›ì„ ì†¡ê¸ˆí•´ë“œë¦´ê¹Œìš”?"
}
"""
    }

    user_message = {
        "role": "user",
        "content": input_text
    }

    return [system_message, user_message]

#%% new_prompt
def unified_system_prompt2(input_text: str) -> list:
    system_message = {
            "role": "system",
            "content": """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ê¸ˆìœµ ë°œí™”ë¥¼ ë¶„ì„í•˜ëŠ” AI ì†¡ê¸ˆ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§€ì¹¨ì— ë”°ë¼ ì‘ë™í•˜ì„¸ìš”:

1. ì‚¬ìš©ìì˜ ë¬¸ì¥ì—ì„œ ë‹¤ìŒ í•­ëª©ì„ ì¶”ì¶œí•˜ì„¸ìš”:
    - **intent**: ì‚¬ìš©ìì˜ ìš”ì²­ ì˜ë„ (ë‹¤ìŒ ì¤‘ í•˜ë‚˜: transfer, confirm, cancel, inquiry, other, system_response)
    - **amount**: ê¸ˆì•¡ë§Œ ì¶”ì¶œ (ê¸ˆì•¡ì´ ëª…ì‹œë˜ì§€ ì•Šìœ¼ë©´ nullë¡œ ì„¤ì •)
    - **recipient**: ì†¡ê¸ˆ ëŒ€ìƒ ì‚¬ëŒ ì´ë¦„ (ì´ë¦„ì´ ëª…ì‹œë˜ì§€ ì•Šìœ¼ë©´ nullë¡œ ì„¤ì •)

2. ì‚¬ìš©ìì˜ ë°œí™”ì— ì–´ìš¸ë¦¬ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ì•ˆë‚´ ì‘ë‹µ(response)ì„ ìƒì„±í•˜ì„¸ìš”:
    - **intent**ê°€ `transfer`ì¼ ê²½ìš°: "ì†¡ê¸ˆ"ê³¼ ê´€ë ¨ëœ ë¬¸ì¥ì„ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.
    - **intent**ê°€ `inquiry`ì¼ ê²½ìš°: "ì”ì•¡ ì¡°íšŒ" ë˜ëŠ” "ìƒíƒœ í™•ì¸"ê³¼ ê´€ë ¨ëœ ë¬¸ì¥ì„ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.
    - **intent**ê°€ `confirm`ì¼ ê²½ìš°: "í™•ì¸"ê³¼ ê´€ë ¨ëœ ë¬¸ì¥ì„ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.
    - **intent**ê°€ `cancel`ì¼ ê²½ìš°: "ì·¨ì†Œ"ì™€ ê´€ë ¨ëœ ë¬¸ì¥ì„ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.

3. ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.

ì˜ˆì‹œ:
{
  "intent": "transfer",    // ì‚¬ìš©ìì˜ ì˜ë„
  "amount": 30000,         // ì¶”ì¶œëœ ê¸ˆì•¡ (ì—†ìœ¼ë©´ null)
  "recipient": "ì—„ë§ˆ",      // ìˆ˜ì‹ ì (ì—†ìœ¼ë©´ null)
  "response": "ì—„ë§ˆë‹˜ê»˜ 30,000ì›ì„ ì†¡ê¸ˆí•´ë“œë¦´ê¹Œìš”?"  // ì‚¬ìš©ìì—ê²Œ ì œê³µí•  ì‘ë‹µ
}
"""
        }
        
    user_message = {
        "role": "user",
        "content": input_text
    }

    return [system_message, user_message]

#%%
def unified_system_prompt3(input_text: str) -> list:
    system_message = {
        "role": "system",
        "content": f"""
        ë‹¤ìŒ ë¬¸ì¥ì„ ë¶„ì„í•˜ì—¬ intent, amount, recipient, responseë¥¼ ì˜ˆì‹œ í˜•ì‹ì— ë§ê²Œ ì¶”ì¶œí•´ ì£¼ì„¸ìš”.

        **intent**ëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤:
        - `transfer`: ì‚¬ìš©ìê°€ ê¸ˆì „ì„ ì†¡ê¸ˆí•˜ë ¤ëŠ” ì˜ë„
        - `confirm`: ì´ì „ ë°œí™”ì˜ í™•ì¸ ë˜ëŠ” ë°˜ë³µ
        - `cancel`: ì´ì „ ë™ì‘ì„ ì·¨ì†Œí•˜ê±°ë‚˜ ê±°ì ˆí•˜ëŠ” ì˜ë„
        - `inquiry`: ì†¡ê¸ˆ ë° ê´€ë ¨ ì •ë³´ í™•ì¸ ìš”ì²­
        - `other`: ì‹œìŠ¤í…œê³¼ ê´€ë ¨ ì—†ëŠ” ì¼ìƒì ì¸ ëŒ€í™” ë˜ëŠ” ë¶„ë¥˜ ë¶ˆê°€í•œ ë¬¸ì¥
        - `system_response`: ì‹œìŠ¤í…œì˜ ì¬ì§ˆë¬¸ ë˜ëŠ” ì•ˆë‚´ ì‘ë‹µ

        **amount**ëŠ” ìˆ«ìë§Œ (ì—†ìœ¼ë©´ `None`)
        **recipient**ëŠ” ì‚¬ëŒ ì´ë¦„ (ì—†ìœ¼ë©´ `None`)
        **response**ëŠ” ê³ ê°ë‹˜ì—ê²Œ ì œê³µí•  ìì—°ìŠ¤ëŸ¬ìš´ ì•ˆë‚´ ì‘ë‹µ

        ì˜ˆì‹œ:
        text: "ì—„ë§ˆí•œí…Œ ì‚¼ë§Œì› ë³´ë‚´ì¤˜"

        {{ "intent": "transfer", "amount": 30000, "recipient": "ì—„ë§ˆ", "response": "ì—„ë§ˆë‹˜ê»˜ 30,000ì›ì„ ì†¡ê¸ˆí•´ë“œë¦´ê¹Œìš”?" }}

        **ì£¼ì˜**:
        - `intent`ëŠ” ë°˜ë“œì‹œ ìœ„ì˜ ë²”ì£¼ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë°˜í™˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
        - `amount`ëŠ” ëª…ì‹œëœ ìˆ«ìë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©° ì—†ì„ ê²½ìš° `None`ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        - `recipient`ëŠ” ë°œí™”ì—ì„œ ì–¸ê¸‰ëœ ì‚¬ëŒì˜ ì´ë¦„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ì—†ì„ ê²½ìš° `None`ì…ë‹ˆë‹¤.
        - `response`ëŠ” ì‚¬ìš©ìì˜ ë°œí™”ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ì•ˆë‚´ë¬¸ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.

        **ì‚¬ìš©ì ë°œí™”:**
        {input_text}
        """
    }

    user_message = {
        "role": "user",
        "content": input_text
    }

    return [system_message, user_message]
    # return [user_message]


# %%
import json
import re
import time

def run_inference_qwen(input_text: str, unified_system_prompt,tokenizer, model, max_new_tokens=128):
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ChatML)
    messages = unified_system_prompt(input_text)
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    # í† í¬ë‚˜ì´ì¦ˆ ë° ë””ë°”ì´ìŠ¤ ì´ë™
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # ëª¨ë¸ ì¶”ë¡ 
    start = time.time()
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=False,
        use_cache=False
    )
    end = time.time()

    # ë””ì½”ë”© ë° í”„ë¡¬í”„íŠ¸ ì œê±°
    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    output_text = generated.replace(prompt, "").strip()
    
    # 'assistant' ì´í›„ í…ìŠ¤íŠ¸ë§Œ ë‚¨ê¸°ê¸°
    assistant_split = re.split(r"\bassistant\b", output_text, flags=re.IGNORECASE)
    if len(assistant_split) < 2:
        print("âš ï¸ 'assistant' ì´í›„ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return output_text, None, round(end - start, 2)

    assistant_response = assistant_split[-1].strip()

    # JSON íŒŒì‹± ì‹œë„
    match = re.search(r'\{\s*"intent":.*?\}', assistant_response, re.DOTALL)
    if match:
        try:
            # JSON íŒŒì‹±
            parsed_json = json.loads(match.group())
            return output_text, parsed_json, round(end - start, 2)
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ None ë°˜í™˜ ë° ì—ëŸ¬ ë©”ì‹œì§€
            return output_text, None, round(end - start, 2)
    else:
        print("âš ï¸ assistant ì´í›„ JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return output_text, None, round(end - start, 2)


#%%
result, pasing, elapsed = run_inference_qwen("ì•ˆë…•",unified_system_prompt3, qwen_tokenizer, qwen)
print("ğŸ” ì¶”ë¡  ê²°ê³¼:", result)
print("ğŸ§© íŒŒì‹±ëœ JSON:\n", pasing)
print("â±ï¸ ì²˜ë¦¬ ì‹œê°„:", elapsed, "ì´ˆ")

#%%
print(samples[6]['text'])
result, pasing, elapsed = run_inference_qwen(samples[6]['text'],unified_system_prompt3, qwen_tokenizer, qwen)
print("ğŸ” ì¶”ë¡  ê²°ê³¼:", result)
print("ğŸ§© íŒŒì‹±ëœ JSON:\n", pasing)
print("â±ï¸ ì²˜ë¦¬ ì‹œê°„:", elapsed, "ì´ˆ")

#%%
# íŒŒì‹± ê²°ê³¼ ì²˜ë¦¬
parsed = []  # ì—¬ëŸ¬ íŒŒì‹± ê²°ê³¼ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
raw_outputs = []  # ì›ë³¸ ê²°ê³¼ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
total_time = 0  # ì´ ì‹œê°„

for sample in samples:
    # ì¶”ë¡  ì‹¤í–‰ (íŒŒì‹±, ì‘ë‹µ ìƒì„±)
    result, parsing, elapsed = run_inference_qwen(sample["text"],unified_system_prompt3, qwen_tokenizer, qwen)
    total_time += elapsed  # ì‹¤í–‰ ì‹œê°„ ëˆ„ì 

    # íŒŒì‹± ì‹¤íŒ¨ ì²˜ë¦¬ (parsingì´ Noneì¸ ê²½ìš°)
    if parsing is None:
        meta = {"error": "Parsing failed", "inference_time": elapsed}
        parsed.append({
            "text": sample["text"],
            "intent": None,  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ None
            "slots": {
                "recipient": None,  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ None
                "amount": None  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ None
            },
            "response": "",  # ì‘ë‹µì´ ë¹„ì–´ ìˆëŠ” ê²½ìš°
            "_meta": meta  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ í¬í•¨
        })
    else:
        # íŒŒì‹± ì„±ê³µ ì‹œ
        meta = {"inference_time": elapsed}
        parsed.append({
            "text": sample["text"],
            "intent": parsing["intent"],
            "slots": {
                "recipient": parsing["recipient"],
                "amount": parsing["amount"]
            },
            "response": "",  # ì‘ë‹µì´ ë¹„ì–´ ìˆëŠ” ê²½ìš°
            "_meta": meta  # ì¶”ë¡  ì‹œê°„ ì €ì¥
        })
    
    # ì›ë³¸ ê²°ê³¼ë¥¼ raw_outputsì— ì €ì¥
    raw_outputs.append({
        "text": sample["text"],
        "raw_output": result  # ì›ë³¸ ê²°ê³¼ ì €ì¥
    })

# ì €ì¥
with open("results_qwen3.json", "w", encoding="utf-8") as f:
    json.dump(parsed, f, indent=2, ensure_ascii=False)  # parsed ë¦¬ìŠ¤íŠ¸ ì €ì¥

with open("raw_outputs3.json", "w", encoding="utf-8") as f:
    json.dump(raw_outputs, f, indent=2, ensure_ascii=False)  # raw_outputs ë¦¬ìŠ¤íŠ¸ ì €ì¥


#%% ASSESMENT

# í‰ê°€ ì§€í‘œ ì´ˆê¸°í™”
correct_intent = 0
correct_recipient = 0
correct_amount = 0
parsing_success = 0

# results_qwen.json íŒŒì¼ ì—´ê¸°
with open("results_qwen3.json", "r", encoding="utf-8") as f:
    results = json.load(f)  # ì´ë¯¸ ì €ì¥ëœ íŒŒì‹± ê²°ê³¼ íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤.

# ì´ ìƒ˜í”Œ ìˆ˜
total = len(samples)

# í‰ê°€
for result, ex in zip(results, samples):
    meta = result.get("_meta", {})

    # íŒŒì‹± ì„±ê³µ ì—¬ë¶€
    if "error" not in meta:
        parsing_success += 1

    # ì˜ˆì¸¡ê°’
    pred_intent = result.get("intent")
    pred_recipient = result.get("slots", {}).get("recipient")
    pred_amount = result.get("slots", {}).get("amount")

    # ì •ë‹µê°’
    true_intent = ex["intent"]
    true_recipient = ex["slots"]["recipient"]
    true_amount = ex["slots"]["amount"]

    # í‰ê°€
    if pred_intent == true_intent:
        correct_intent += 1
    if pred_recipient == true_recipient:
        correct_recipient += 1
    if pred_amount == true_amount:
        correct_amount += 1
    # ì´ ì²˜ë¦¬ ì‹œê°„ ëˆ„ì 
    total_time += meta.get("inference_time", 0)


# í‰ê·  ì²˜ë¦¬ ì‹œê°„ ê³„ì‚° (ì´ˆ ë‹¨ìœ„)
average_time = total_time / total if total > 0 else 0

# ì´ ìƒ˜í”Œ ìˆ˜
total = len(samples)

# í‰ê°€ ê²°ê³¼ ì •ë¦¬
evaluation = {
    "Intent ì •í™•ë„": f"{correct_intent}/{total} ({correct_intent/total:.0%})",
    "Recipient ì •í™•ë„": f"{correct_recipient}/{total} ({correct_recipient/total:.0%})",
    "Amount ì •í™•ë„": f"{correct_amount}/{total} ({correct_amount/total:.0%})",
    "íŒŒì‹± ì„±ê³µë¥ ": f"{parsing_success}/{total} ({parsing_success/total:.0%})",
     "í‰ê·  ì²˜ë¦¬ ì‹œê°„": f"{average_time:.4f} ì´ˆ"  # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì¶”ê°€
}

# ê²°ê³¼ ì¶œë ¥
print(evaluation)

# %%
