#%%
!pip install -r requirements.txt

import json
import re
import time
import os
import importlib
from mlx_lm import load, generate

# %% data load
# íŒŒì¼ ì—´ê¸°
with open("/Users/hajin/Projects/MCP_Voice_Transfer/experiments/llms/data/samples.json", "r", encoding="utf-8") as f:
    samples = json.load(f)

with open("/Users/hajin/Projects/MCP_Voice_Transfer/experiments/llms/data/transfer.json", "r", encoding="utf-8") as f:
    transfer = json.load(f)

with open("/Users/hajin/Projects/MCP_Voice_Transfer/experiments/llms/data/non_memory.json", "r", encoding="utf-8") as f:
    non_memory = json.load(f)
#%%func
class Qwen3InferenceEngine:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.history = []

    def reset_history(self):
        self.history = []

    def run_inference(self, user_input, unified_system_prompt, max_new_tokens=2048, thinking_mode=True):
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        user_prompt_text = user_input
        system_prompt = unified_system_prompt(user_input)
        self.history = system_prompt  # system_message í¬í•¨

        self.history.append({"role": "user", "content": user_prompt_text})

        # chat template ì ìš©
        prompt = self.tokenizer.apply_chat_template(
            self.history,
            tokenize=False,
            add_generation_prompt=True
        )

        start = time.time()
        response = generate(
            self.model,
            self.tokenizer,
            prompt=prompt,
            max_tokens=max_new_tokens,
            verbose=False
        )
        end = time.time()

        self.history.append({"role": "assistant", "content": response})

        output_text = response.strip()

        match = re.search(r'\{[\s\S]*?\}', output_text)
        if match:
            try:
                parsed_json = json.loads(match.group())
                return output_text, parsed_json, round(end - start, 2)
            except json.JSONDecodeError as e:
                print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        else:
            print("âš ï¸ JSON íŒ¨í„´ ì°¾ê¸° ì‹¤íŒ¨")

        return output_text, None, round(end - start, 2)


#%%
def evaluate_results(results, samples, total_time):
    correct_intent = 0
    correct_recipient = 0
    correct_amount = 0
    parsing_success = 0

    total = len(samples)

    for result, ex in zip(results, samples):
        meta = result.get("_meta", {})
        if "error" not in meta:
            parsing_success += 1

        if result.get("intent") == ex["intent"]:
            correct_intent += 1
        if result.get("slots", {}).get("recipient") == ex["slots"]["recipient"]:
            correct_recipient += 1
        if result.get("slots", {}).get("amount") == ex["slots"]["amount"]:
            correct_amount += 1

        total_time += meta.get("inference_time", 0)

    average_time = total_time / total if total > 0 else 0

    return {
        "Intent ì •í™•ë„": f"{correct_intent}/{total} ({correct_intent/total:.0%})",
        "Recipient ì •í™•ë„": f"{correct_recipient}/{total} ({correct_recipient/total:.0%})",
        "Amount ì •í™•ë„": f"{correct_amount}/{total} ({correct_amount/total:.0%})",
        "íŒŒì‹± ì„±ê³µë¥ ": f"{parsing_success}/{total} ({parsing_success/total:.0%})",
        "í‰ê·  ì²˜ë¦¬ ì‹œê°„": f"{average_time:.4f} ì´ˆ"
    }

#%% === ì˜ˆì‹œ ì‚¬ìš© ===
if __name__ == "__main__":
    from prompts import unified_system_prompt0 
    model, tokenizer = load("Qwen/Qwen3-0.6B-MLX-8bit")
    engine = Qwen3InferenceEngine(model, tokenizer)

    test_input = "ì—„ë§ˆí•œí…Œ ì‚¼ë§Œ ì› ë³´ë‚´ì¤˜"
    result, parsed_json, elapsed = engine.run_inference(test_input, unified_system_prompt0, thinking_mode=True)

    print("ğŸ” ì¶”ë¡  ê²°ê³¼:", result)
    print("ğŸ§© íŒŒì‹±ëœ JSON:", parsed_json)
    print("â±ï¸ ì²˜ë¦¬ ì‹œê°„:", elapsed, "ì´ˆ")
#%% ì¸í¼ëŸ°ìŠ¤ ì‹œì‘
import json
import os
import importlib

model_name = "Qwen/Qwen3-1.7B-MLX-8bit"

model, tokenizer = load(model_name)
engine = Qwen3InferenceEngine(model, tokenizer)

results_summary = {}
prompt_module = importlib.import_module("prompts")

for i in range(1, 7):
    prompt_name = f"unified_system_prompt{i}"
    prompt_fn = getattr(prompt_module, prompt_name, None)

    if prompt_fn is None:
        print(f"âŒ {prompt_name} í•¨ìˆ˜ ì—†ìŒ")
        continue

    print(f"\nâœ… ì‹¤í–‰ ì¤‘: {prompt_name}")

    parsed_results = []
    raw_outputs = []
    total_time = 0

    for sample in samples:
        result, parsing, elapsed = engine.run_inference(sample["text"], prompt_fn, thinking_mode=True)
        total_time += elapsed

        raw_outputs.append({
            "text": sample["text"],
            "raw_output": result
        })

        if parsing is None:
            parsed_results.append({
                "text": sample["text"],
                "intent": None,
                "slots": {"recipient": None, "amount": None},
                "response": "",
                "_meta": {"error": "Parsing failed", "inference_time": elapsed}
            })
        else:
            parsed_results.append({
                "text": sample["text"],
                "intent": parsing["intent"],
                "slots": {
                    "recipient": parsing.get("recipient"),
                    "amount": parsing.get("amount")
                },
                "response": parsing.get("response"),
                "_meta": {"inference_time": elapsed}
            })

    # í‰ê°€ ê²°ê³¼ ì €ì¥
    evaluation = evaluate_results(parsed_results, samples, total_time)
    results_summary[prompt_name] = evaluation

    # ê²½ë¡œ ì„¤ì •
    save_dir = f"results/{model_name}/{prompt_name}"
    os.makedirs(save_dir, exist_ok=True)

    # ì €ì¥
    with open(os.path.join(save_dir, "parsed.json"), "w", encoding="utf-8") as f:
        json.dump(parsed_results, f, indent=2, ensure_ascii=False)

    with open(os.path.join(save_dir, "raw_outputs.json"), "w", encoding="utf-8") as f:
        json.dump(raw_outputs, f, indent=2, ensure_ascii=False)

# ì „ì²´ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
print("\nğŸ“Š ì „ì²´ í‰ê°€ ìš”ì•½")
for name, evaluation in results_summary.items():
    print(f"\nğŸ”¹ {name}")
    for metric, value in evaluation.items():
        print(f"  - {metric}: {value}")

#%% ì¸í¼ëŸ°ìŠ¤ ì‹œì‘
import json
import os
import importlib

model_name = "Qwen/Qwen3-0.6B-MLX-8bit"

model, tokenizer = load(model_name)
engine = Qwen3InferenceEngine(model, tokenizer)

results_summary = {}
prompt_module = importlib.import_module("prompts")

for i in range(1, 7):
    prompt_name = f"unified_system_prompt{i}"
    prompt_fn = getattr(prompt_module, prompt_name, None)

    if prompt_fn is None:
        print(f"âŒ {prompt_name} í•¨ìˆ˜ ì—†ìŒ")
        continue

    print(f"\nâœ… ì‹¤í–‰ ì¤‘: {prompt_name}")

    parsed_results = []
    raw_outputs = []
    total_time = 0

    for sample in samples:
        result, parsing, elapsed = engine.run_inference(sample["text"], prompt_fn, thinking_mode=True)
        total_time += elapsed

        raw_outputs.append({
            "text": sample["text"],
            "raw_output": result
        })

        if parsing is None:
            parsed_results.append({
                "text": sample["text"],
                "intent": None,
                "slots": {"recipient": None, "amount": None},
                "response": "",
                "_meta": {"error": "Parsing failed", "inference_time": elapsed}
            })
        else:
            parsed_results.append({
                "text": sample["text"],
                "intent": parsing["intent"],
                "slots": {
                    "recipient": parsing.get("recipient"),
                    "amount": parsing.get("amount")
                },
                "response": parsing.get("response"),
                "_meta": {"inference_time": elapsed}
            })

    # í‰ê°€ ê²°ê³¼ ì €ì¥
    evaluation = evaluate_results(parsed_results, samples, total_time)
    results_summary[prompt_name] = evaluation

    # ê²½ë¡œ ì„¤ì •
    save_dir = f"results/{model_name}/{prompt_name}"
    os.makedirs(save_dir, exist_ok=True)

    # ì €ì¥
    with open(os.path.join(save_dir, "parsed.json"), "w", encoding="utf-8") as f:
        json.dump(parsed_results, f, indent=2, ensure_ascii=False)

    with open(os.path.join(save_dir, "raw_outputs.json"), "w", encoding="utf-8") as f:
        json.dump(raw_outputs, f, indent=2, ensure_ascii=False)

# ì „ì²´ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
print("\nğŸ“Š ì „ì²´ í‰ê°€ ìš”ì•½")
for name, evaluation in results_summary.items():
    print(f"\nğŸ”¹ {name}")
    for metric, value in evaluation.items():
        print(f"  - {metric}: {value}")

# %%
