# %% library load
!pip install -r requirements.txt
import sys
sys.path.append("/Users/hajin/Projects/MCP_Voice_Transfer/experiments/llms")
import os
import json
import importlib
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from runner import llm_sampling, run_inference, evaluate_results,run_inference_qwen3
from mlx_lm import load, generate

# %% data load
# íŒŒì¼ ì—´ê¸°
with open("/Users/hajin/Projects/MCP_Voice_Transfer/experiments/llms/data/samples.json", "r", encoding="utf-8") as f:
    samples = json.load(f)

with open("/Users/hajin/Projects/MCP_Voice_Transfer/experiments/llms/data/transfer.json", "r", encoding="utf-8") as f:
    transfer = json.load(f)

with open("/Users/hajin/Projects/MCP_Voice_Transfer/experiments/llms/data/non_memory.json", "r", encoding="utf-8") as f:
    non_memory = json.load(f)

print(f"ìƒ˜í”Œ ìˆ˜: {len(samples)}")

# %% model load
device = "mps" if torch.backends.mps.is_available() else "cpu"

# # model_name = "Qwen/Qwen2.5-0.5B-Instruct"  
# model = AutoModelForCausalLM.from_pretrained(model_name,
#                                              torch_dtype=torch.float16,
#                                              device_map={"":device}
#                                              )
# tokenizer=AutoTokenizer.from_pretrained(model_name)

model_name="Qwen/Qwen3-1.7B-MLX-8bit"
model, tokenizer = load("Qwen/Qwen3-1.7B-MLX-8bit")


#%%
from prompts import unified_system_prompt1
result, pasing, elapsed = run_inference_qwen3("ì•ˆë…•",unified_system_prompt1, tokenizer, model)
print("ğŸ” ì¶”ë¡  ê²°ê³¼:", result)
print("ğŸ§© íŒŒì‹±ëœ JSON:\n", pasing)
print("â±ï¸ ì²˜ë¦¬ ì‹œê°„:", elapsed, "ì´ˆ")

print(samples[3]['text'])
result, pasing, elapsed = run_inference_qwen3(samples[3]['text'],unified_system_prompt1, tokenizer, model)
print("ğŸ” ì¶”ë¡  ê²°ê³¼:", result)
print("ğŸ§© íŒŒì‹±ëœ JSON:\n", pasing)
print("â±ï¸ ì²˜ë¦¬ ì‹œê°„:", elapsed, "ì´ˆ")


#%%
results_summary={}
prompt_module = importlib.import_module("prompts")
for i in range(1,7):
    prompt_name=f"unified_system_prompt{i}"
    prompt_fn = getattr(prompt_module, prompt_name, None)
    
    if prompt_fn is None:
        print(f"âŒ {prompt_name} í•¨ìˆ˜ ì—†ìŒ")
        continue
    
    print(f"âœ… ì‹¤í–‰ ì¤‘: {prompt_name}")
    evaluation =llm_sampling(model,tokenizer, samples, prompt_fn)
    results_summary[prompt_name] = evaluation

 
 
#%% ASSESMENT

 
# ì „ì²´ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
print("\nğŸ“Š ì „ì²´ í‰ê°€ ìš”ì•½")
for name, evaluation in results_summary.items():
    print(f"\nğŸ”¹ {name}")
    for metric, value in evaluation.items():
        print(f"  - {metric}: {value}")


# %%
