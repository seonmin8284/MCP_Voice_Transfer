import 'dart:io';
import 'package:flutter/services.dart';
import 'package:path_provider/path_provider.dart';
import 'package:llama_cpp_dart/llama_cpp_dart.dart';

import 'dart:io';
import 'package:flutter/foundation.dart';

//ì–¸ì–´ëª¨ë¸ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
class QwenPromptFormat extends PromptFormat {
  QwenPromptFormat()
    : super(
        PromptFormatType.chatml,
        inputSequence: "<|im_start|>user\n",
        outputSequence: "<|im_end|>\n<|im_start|>assistant\n",
        systemSequence: "<|im_start|>system\n",
        stopSequence: "<|im_end|>",
      );

  @override
  String formatPrompt(String prompt) {
    return """
<|im_start|>system
You are a helpful assistant.
<|im_end|>
<|im_start|>user
$prompt
<|im_end|>
<|im_start|>assistant
""";
  }
}

Future<String> downloadQwenModel({
  required String modelName,
  required String destinationPath,
}) async {
  // Hugging Face ëª¨ë¸ URL êµì²´
  final url =
      "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/$modelName";

  final httpClient = HttpClient();
  final request = await httpClient.getUrl(Uri.parse(url));
  final response = await request.close();

  final file = File('$destinationPath/$modelName');
  final raf = file.openSync(mode: FileMode.write);

  await for (var chunk in response) {
    raf.writeFromSync(chunk);
  }

  await raf.close();

  if (kDebugMode) {
    debugPrint("âœ… Qwen ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: ${file.path}");
  }

  return file.path;
}

class NluService {
  late final LlamaParent _llamaParent;
  late final String _localModelPath;
  final StringBuffer _responseBuffer = StringBuffer();
  Stream<String> get stream => _llamaParent.stream;
  Stream<void> get completions => _llamaParent.completions;
  NluService();

  Future<void> _prepareModel() async {
    final directory = await getApplicationDocumentsDirectory();
    final modelName = 'qwen2.5-0.5b-instruct-q2_k.gguf';
    final file = File('${directory.path}/$modelName');

    if (!(await file.exists())) {
      print('ğŸŒ Qwen ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...');
      _localModelPath = await downloadQwenModel(
        modelName: modelName,
        destinationPath: directory.path,
      );
    } else {
      print('âš¡ ì´ë¯¸ ëª¨ë¸ íŒŒì¼ ì¡´ì¬: ${file.path}');
      _localModelPath = file.path;
    }
  }

  Future<void> initialize() async {
    await _prepareModel();

    final loadCommand = LlamaLoad(
      path: _localModelPath,
      modelParams: ModelParams(),
      contextParams: ContextParams(),
      samplingParams: SamplerParams(), // ì¶œë ¥ í† í° ì œí•œ ì¶”ê°€
      format: QwenPromptFormat(),
    );

    _llamaParent = LlamaParent(loadCommand);
    await _llamaParent.init();
    print("ğŸŸ¢ [NLU Init] ëª¨ë¸ ì„¸ì…˜ ë¡œë”© ì„±ê³µ!");

    _llamaParent.stream.listen((response) {
      if (response.trim().isEmpty) {
        print('ğŸ§  (ê²½ê³ ) ë¹ˆ ì‘ë‹µ ìˆ˜ì‹ !');
      } else {
        _responseBuffer.write(response); // ëˆ„ì 
        print('ğŸ§  ëª¨ë¸ ì‘ë‹µ ìŠ¤íŠ¸ë¦¼ ìˆ˜ì‹ : "$response"');
      }
    });

    _llamaParent.completions.listen((event) {
      print('ğŸ“¥ Completion ì™„ë£Œë¨: $event');
      print('ğŸ’¬ ì „ì²´ ì‘ë‹µ ê²°ê³¼: ${_responseBuffer.toString()}');
      _responseBuffer.clear(); // ë‹¤ìŒ ì‘ë‹µ ìœ„í•´ ì´ˆê¸°í™”
    });
  }

  void ask(String inputText) {
    final prompt = QwenPromptFormat().formatPrompt(inputText);
    print('ğŸ“¨ ì‹¤ì œ ì „ì†¡ë  Prompt: $prompt');
    _llamaParent.sendPrompt(prompt);
  }
}
